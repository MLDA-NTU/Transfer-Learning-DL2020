{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Transfer Learning (Exercise).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Uj7mwRv7tQYL"},"source":["# Transfer Learning for Image Classification with TensorFlow\n","\n","**Author**: Thien Tran (tran0096@e.ntu.edu.sg)\n","\n","**Last update**: 13 October 2020\n","\n","This notebook is part of MLDA@EEE's series of workshops during the Deep Learning week 2020.\n","\n","Designed to run in Google Colab."]},{"cell_type":"markdown","metadata":{"id":"OJlCRKRYMWyp"},"source":["## Connect to GPU instance\n","To connect to GPU instance on Google Colab, follow the instruction below\n","- Edit > Notebook settings > Hardware accelerator > GPU"]},{"cell_type":"markdown","metadata":{"id":"kRp_9Bacp-_f"},"source":["# Notebook outline\n","1. Download data from Kaggle\n","2. Choose a pre-trained model\n","3. Data preparation\n","4. Model training\n","5. Model evaluation\n","6. Test with your own image\n","7. Train with small dataset\n","8. Visualize feature vectors"]},{"cell_type":"markdown","metadata":{"id":"uqAxRLtMtQYN"},"source":["## Download data \n","\n","We are using `Intel Image Classification` dataset from Kaggle\n","\n","Data source: https://www.kaggle.com/puneet6060/intel-image-classification"]},{"cell_type":"code","metadata":{"id":"G_Gz5jFrtQYP"},"source":["# install kaggle CLI to download the dataset from Kaggle\n","!pip install -q kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79FoqqNZtQYT"},"source":["### Task\n","- Generate your Kaggle token under My account > API > Create New API Token\n","- Upload your Kaggle token `kaggle.json` to the Google Colab instance"]},{"cell_type":"code","metadata":{"id":"Chqou0AmtQYU"},"source":["# copy token to .kaggle/ folder and set appropriate permission\n","!mkdir ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9dnlYkDGtQYX"},"source":["!kaggle datasets download -d puneet6060/intel-image-classification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlyjWp0-tQYa"},"source":["# Unzip the zip file with unzip\n","!unzip -q 'intel-image-classification'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RFD19OuQtQYk"},"source":["The dataset is downloaded and unzipped"]},{"cell_type":"code","metadata":{"id":"7V2CFphTtQYn"},"source":["import numpy as np                  # numerical calculations\n","import pandas as pd                 # organize data in DataFrame (table)\n","import matplotlib.pyplot as plt     # visualize data\n","import seaborn as sns\n","\n","import tensorflow as tf             # deep learning framework\n","import tensorflow_hub as hub        # access pre-trained models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg7sQBvCcQdu"},"source":["# Choose model architecture\n","\n","The model you choose must correspond to the task (**Multi-class Image Classification**) and has good performance. The size of the model should also be suitable to the size of our training set (~14,000 images).\n","\n","## Load a pre-trained model\n","\n","There are many ways load pre-trained models in TensorFlow. Two recommended ways are\n","1. Import a pre-trained model from TensorFlow hub as a Keras layer\n","2. Load a pre-trained model from Keras `tf.keras.applications`\n","\n","You also need to specify the input image dimensions to create the model. You can find this information on the page you obtain the pre-trained model.\n","\n","> Link to TensorFlow Hub: https://tfhub.dev/\n","\n","> List of pre-trained models in Keras: https://www.tensorflow.org/api_docs/python/tf/keras/applications\n","\n","From my personal experience, the same model architecture in Keras performs worse compared to TensorFlow Hub's, in both accuracy and training speed. Thus I personally recommend TensorFlow Hub over Keras for obtainingpre-trained models.\n","\n","**NOTE**: you can find many people re-implement neural network architecture online and can be downloaded from PyPI. However, it is not recommended to do so, because there might be mistakes in implementation and missing pre-trained weights. Thus it is only recommnded to use pre-trained models from official and established sources "]},{"cell_type":"markdown","metadata":{"id":"FV-5cr58cQdv"},"source":["### Task\n","- Choose a pre-trained model from TensorFlow Hub. Use the `feature-vector` version, not the `classification` one\n","- Follow the provided instructions to create a pre-trained model\n","- Specify the correct image dimension for your chosen pre-trained model\n","- Specify the number of classes\n","\n","Use `model.summary()` to see summary of your model."]},{"cell_type":"code","metadata":{"id":"EWj8zxkSqVNw"},"source":["# Load pre-trained model from TensorFlow Hub\n","hub_url = <YOUR CODE HERE>\n","hub_model = hub.KerasLayer(hub_url, trainable=False)\n","\n","hub_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mDFzdf_tp0J"},"source":["> To have better numerical stability during training, it's a common practice to not use softmax activation at the last classification layer (dense layer) and indicate `from_logits=True` in the loss function (see Model training later)"]},{"cell_type":"code","metadata":{"id":"Qo-39_H2cQdy"},"source":["# parameters to build the model\n","IMG_WIDTH = <YOUR CODE HERE>\n","IMG_HEIGHT = <YOUR CODE HERE>\n","IMG_DIM = (IMG_WIDTH, IMG_HEIGHT)\n","\n","NUM_CLASSES = <YOUR CODE HERE>\n","\n","model = tf.keras.Sequential([\n","    <YOUR CODE HERE>\n","])\n","model.build([None, IMG_HEIGHT, IMG_WIDTH, 3])                   # let the model know what's the input shape looks like\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CnDWz5nYtQYq"},"source":["# Data Preparation\n","\n","Your pre-trained model expects image input to be in certain format and sizes. Check with the site where you obtain the pre-trained model to prepare the data correctly.\n","> e.g. Resnet V1 50 on TensorFlow Hub expects images to have **values** within `[0, 1]` and of the **size** `224 x 224` pixels\n","\n","We have the following folder structure (see `seg_train/seg_train`)\n","```\n","class1\n","    |---10001.jpg\n","    |---10002.jpg\n","    ...\n","class2\n","    |---20001.jpg\n","    |---20002.jpg\n","    ...\n","```\n","\n","Keras `image_dataset_from_directory` provides a convenient way to import images with this folder structure. It only loads images when needed, so we don't need to load the whole image dataset into memory."]},{"cell_type":"markdown","metadata":{"id":"yXOfRLfBcQd1"},"source":["### Task\n","- Write down the path to train and test folder"]},{"cell_type":"code","metadata":{"id":"6Bz--3P7cQd2"},"source":["TRAIN_PATH = <YOUR CODE HERE>\n","TEST_PATH = <YOUR CODE HERE>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SgDOj6YcQd4"},"source":["## image_dataset_from_directory()\n","\n","We can use the function `image_dataset_from_directory()` to load the images in the train and test directories. Important parameters are:\n","- `directory`: where to look for images\n","- `batch_size`: size of 1 batch of the data (see below)\n","- `image_size`: this is the image dimension as you have defined above\n","- `shuffle`: whether TensorFlow should shuffle the data (You shouldn't shuffle the test set)\n","\n","### About batching\n","We can't load the whole dataset to memory, so we have to divide the dataset into batches. \n","- Batch size must be a power of 2 (e.g. 16, 32, 64) to fit in GPU memory nicely\n","- The larger the batch size, the faster the training process\n","- Rule of thumb is to choose the largest batch size that can fit into your GPU memory\n","\n","**NOTE**: If later at training stage, you face Out of Memory error, you should go back at this step and reduce the batch size. \n","\n","You can read more about other parameters here: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory\n","\n","### Task\n","- Create train and test set using `image_dataset_from_directory()` and specify the correct arguments"]},{"cell_type":"code","metadata":{"id":"jt7vSQe2cQd4"},"source":["BATCH_SIZE = <YOUR CODE HERE>\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    <YOUR CODE HERE>\n",")\n","\n","test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    <YOUR CODE HERE>\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qak2VIAQcQd6"},"source":["We can save the classes that TensorFlow has discovered in a separate variable from the attribute `.class_names`"]},{"cell_type":"code","metadata":{"id":"wecrRGQLcQd7"},"source":["CLASSES = train_ds.class_names\n","CLASSES"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hIkKGoldqgDE"},"source":["## TensorFlow Dataset API\n","\n","TensorFlow Dataset is a computational graph, meaning the values and operations are not calculated until they are needed. We can use the method `.map()` to apply some operations and transformations to the dataset. Two operations we will apply here:\n","- Normalize the image values to `[0,1]` (most TensorFlow Hub models require this)\n","- Apply augmentation to training data\n","\n","**NOTE**: Since TensorFlow Dataset builds a computational graph, you MUST use TensorFlow functions and operations inside the `.map()` method\n","\n","> `.prefetch()` is an optimization feature of TensorFlow Dataset. TensorFlow will start import and prepare the next batch of images while the previous batch is being used for training. Thus this will reduce I/O blocking."]},{"cell_type":"code","metadata":{"id":"3DtjbhetcQd9"},"source":["def normalize(imgs, labels):\n","    # convert [0,255] to [0,1]\n","    imgs = imgs/255\n","    return imgs, labels\n","    \n","def augment(imgs, labels):\n","    # here I apply only 2 augmentation methods: random flip left right and random crop\n","    imgs = tf.image.random_flip_left_right(imgs)\n","\n","    # scale image up to (IMG_WIDTH*1.5) x (IMG_HEIGHT*1.5) then crop back the original size\n","    new_width = int(IMG_WIDTH*1.5)\n","    new_height = int(IMG_HEIGHT*1.5)\n","    imgs_shape = tf.shape(imgs)\n","    imgs = tf.image.resize(imgs, (new_width, new_height))\n","    imgs = tf.image.random_crop(imgs, imgs_shape)\n","\n","    return imgs, labels\n","\n","train_ds = train_ds.map(normalize, num_parallel_calls=AUTOTUNE).map(augment, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n","test_ds = test_ds.map(normalize, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vqFBsJY9s5b1"},"source":["We can use the method `.take(n)` to take `n` batches of data from the dataset. Then we can visualize the images together with their corresponding labels.\n","\n","The class label number corresponds to the position in the `CLASSES` list."]},{"cell_type":"code","metadata":{"id":"rGYQSEm1cQeA"},"source":["plt.figure(figsize=(24,10))\n","\n","for imgs, labels in train_ds.take(1):\n","    # take 1 batch. each batch has BATCH_SIZE samples\n","    for i in range(8):\n","        plt.subplot(2,4,i+1)\n","        image = imgs[i]\n","        label = labels[i]\n","        plt.imshow(image)\n","        plt.axis('off')\n","        plt.title('Label ' + str(label.numpy()))\n","        \n","print(CLASSES)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Io-Qrg9bhyT"},"source":["## (Advanced topic) Image augmentation\n","Image augmentation plays a crucial part in neural network training and transfer learning. It prevents overfitting and helps generating more data when dataset is limited.\n","\n","Some modern, State-of-the-Art image augmentation techniques include:\n","\n","Technique | Year published | Link\n","----------|----------------|------\n","Cutout | 2017 | https://arxiv.org/abs/1708.04552\n","Mixup | 2017 | https://arxiv.org/pdf/1710.09412.pdf\n","Hide and Seek | 2018 | https://arxiv.org/abs/1811.02545\n","CutMix | 2019 | https://arxiv.org/pdf/1905.04899.pdf\n","GridMask | 2020 | https://arxiv.org/abs/2001.04086\n","\n","Cutout is currently available in TensorFlow Addons nightly build [here](https://www.tensorflow.org/addons/api_docs/python/tfa/image/random_cutout). To use other augmentation techniques in TensorFlow, you have to implement them yourself (or Google someone who has done it)\n","\n","<br>\n","\n","Key considerations in using Image augmentation\n","- **Speed**: apply too many augmentations can slow down the training process significantly, because the CPU/GPU has to apply transformations to the images before feeding to the neural network.\n","- **Effectiveness**: not all augmentation techniques are the same in generalizing the data. "]},{"cell_type":"markdown","metadata":{"id":"hiPJ54xHHoHS"},"source":["# Model training\n","\n","## Compile the model\n","\n","You need a **loss** function and an **optimizer** for TensorFlow to know how to train your model.\n","- The **loss** (sometimes called the cost) is a function that measures how well (or how badly) our model performs with the dataset\n","- The **optimizer** is a mathematical method to minimize the loss, so that our model will become better, hence 'trained'\n","\n","### Losses\n","\n","The loss usually depends on your task and how to prepare your dataset output. Our task is Multi-class Image Classification, so you can find an appropriate loss function for this task.\n","\n","> To provide better numerical stability for the loss, it's a common practice to not use softmax activation at the last classification layer (dense layer) and indicate `from_logits=True` in the loss function\n","\n","<br>\n","\n","### Optimizers\n","\n","Some popular optimizers to choose from are SGD, Adam and RMSprop.\n","- **SGD** is the traditional optimizer. It is slower than Adam and RMSprop, but generally converges to better minima than Adam and RMSprop\n","- **Adam** and **RMSprop** are adaptive learning rate optimizers. They are much faster than SGD.\n","\n","![Visualize optimizers](https://i.imgur.com/NKsFHJb.gif)\n","\n","(Credits to [Alec Radford](http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html). Check out his other cool animations [here](https://imgur.com/s25RsOr)) \n","\n","> You can read more about different types of optimizers here: https://mlfromscratch.com/optimizers-explained/\n","\n","<br>\n","\n","From my personal experience:\n","- If you don't have much time, use **Adam**. Chances are, with a pre-trained model, you only need to train 1-5 epochs to get excellent performance. Note that training more epochs with Adam quickly leads to over-fitting.\n","- If you have enough time, use **SGD**. Although training time is much longer, the better performance from SGD optimizer may be worth it for your use case."]},{"cell_type":"markdown","metadata":{"id":"MjreFAYitQZR"},"source":["### Task\n","\n","- Find and use an optimizer from `tf.keras.optimizers` https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n","- Find and use a loss from `tf.keras.losses` https://www.tensorflow.org/api_docs/python/tf/keras/losses\n","- Default metrics is Accuracy (`acc`). You can add more metrics if you want https://www.tensorflow.org/api_docs/python/tf/keras/metrics"]},{"cell_type":"code","metadata":{"id":"rjxEh8HntQZR"},"source":["model.compile(\n","    optimizer=<YOUR CODE HERE>,\n","    loss=<YOUR CODE HERE>,\n","    metrics=['acc'],\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P32e_XmycQeG"},"source":["## Start the training\n","\n","Information about the training process (e.g. training loss, validation loss) is saved in a `history` object, which is returned by the method `.fit()`. We can access this history object later to inspect the training process"]},{"cell_type":"markdown","metadata":{"id":"V0sKfmOZpj3z"},"source":["### Task\n","- Specify your train and test set\n","- Choose how many epochs you want to train for"]},{"cell_type":"code","metadata":{"id":"osxkuUMytQZT"},"source":["history = model.fit(\n","    <YOUR CODE HERE>,\n","    epochs=<YOUR CODE HERE>,\n","    validation_data=<YOUR CODE HERE>\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZXS7utCCxLW8"},"source":["## Visualize training process"]},{"cell_type":"code","metadata":{"id":"T_38y2o5rjID"},"source":["history_data = pd.DataFrame(history.history)\n","\n","history_data.plot(y=['loss','val_loss'])\n","history_data.plot(y=['acc','val_acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O27Fk9LMPFbL"},"source":["## (Advanced topic) Learning rate scheduler, Hyperparameters search, and early stopping for SGD optimizer\n","\n","**Learning rate scheduler**: we want to decrease the learning rate as we get closer to the minima. This can be achieved with `tf.keras.callbacks.LearningRateScheduler`. Some popular methods are:\n","- Learning rate exponential decay\n","- Learning rate stepped decay\n","- Learning rate warm up (good for transfer learning)\n","\n","**Hyperparameters search**: there are many hyperparameters during training that affect the final model's accuracy, such as learning rate and learning rate schedule. We can try different sets of hyperparameters to find the best combination.\n","- When changing a hyperparameter value, change its order of magnitude e.g. `learning_rate=0.001` and `learning_rate=0.0001`\n","\n","**Early stopping**: when to stop training? How to detect overfitting? We can train the model for many epochs (e.g. 100-200 epochs) to see its long term trends. Plotting the accuracy and loss graph over time, which reveal when the model starts to overfit (training loss still drops but validation loss starts to increase). We can re-train the model up to that epoch to get the best performance"]},{"cell_type":"markdown","metadata":{"id":"lsJR05aGqT0v"},"source":["# Model evaluation - Confusion matrix\n","\n","Get predictions with the method `.predict()`"]},{"cell_type":"code","metadata":{"id":"aKeCdf3fq4aC"},"source":["# get predictions from test set\n","probs = model.predict(test_ds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NgieQcxGu5Uf"},"source":["We can get an array of grouth truth label from the test set by iterating over it and only extract the label part"]},{"cell_type":"code","metadata":{"id":"KzMciw_3cQeP"},"source":["# get labels (ground truth) from test set\n","labels = []\n","\n","for _, label_b in test_ds:\n","    labels.append(label_b)\n","    \n","labels = np.concatenate(labels)\n","print(labels.shape)\n","labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzRHBBs3vAzh"},"source":["If you don't include the softmax activation at the classification layer, the output will be logits instead of probabilities. To convert to probabilities, you just need to apply softmax function to the logits.\n","\n","To get the number label, find position of the largest probability with `np.argmax()`"]},{"cell_type":"code","metadata":{"id":"qFfPHJeHvZIh"},"source":["preds = np.argmax(probs, axis=1)    # get the index of largest probability\n","preds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X4jtN_OAvfSt"},"source":["We can visualize the performance of the model with a confusion matrix. Note that the horizontal axis is **predictions**, while the vertical axis is **ground truth labels**"]},{"cell_type":"code","metadata":{"id":"mf7WImwTTVbv"},"source":["conf_matrix = tf.math.confusion_matrix(labels, preds)\n","\n","plt.figure(figsize=(10,8))\n","heatmap = sns.heatmap(conf_matrix, annot=True, fmt='d', square=True, xticklabels=CLASSES, yticklabels=CLASSES)\n","heatmap.set_xlabel('Predictions')\n","heatmap.set_ylabel('Labels')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KsHWMhhyHs5p"},"source":["# Test with your own image!\n","\n","Upload your own image to the Colab instance. Ideally, use an image from the classes that we have trained.\n","\n","We don't want to use an image of objects that the model hasn't seen before (aka out of distribution). However, feel free to try and see what your model predicts!"]},{"cell_type":"code","metadata":{"id":"bqVfijdAbbhz"},"source":["CLASSES"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRxW-1WAtQZV"},"source":["test_img = <YOUR CODE HERE>\n","test_img = tf.io.read_file(test_img)\n","test_img = tf.image.decode_jpeg(test_img)\n","\n","plt.imshow(test_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBY564ovvKu1"},"source":["# prepare image input before feeding to the model\n","test_img = tf.image.convert_image_dtype(test_img, tf.float32)       # convert from [0,255] to [0,1]\n","test_img = tf.image.resize(test_img, (IMG_WIDTH, IMG_HEIGHT))       # resize\n","test_img = tf.expand_dims(test_img, axis=0)                         # add the batch dimension\n","test_img.shape                                                      # now image has the shape (1, height, width, channel)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tnrPThVVvWtI"},"source":["probs = model.predict(test_img)[0]\n","probs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1GrdAcgugdF"},"source":["prediction = np.argmax(probs)\n","print(prediction)\n","CLASSES[prediction]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCw776IGwdMW"},"source":["table = pd.DataFrame({'class': list(CLASSES), 'probability': probs})\n","table['probability'] = table['probability'].map('{:5f}'.format)\n","table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-4uwpyr0qXC"},"source":["# Train with small dataset\n","\n","Now we will repeat the process, but with only 100 training images, meaning around 17 images per class"]},{"cell_type":"code","metadata":{"id":"dpIiFjjYuwXf"},"source":["train100_ds = train_ds.take(100//BATCH_SIZE+1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOtWIJB-tQZZ"},"source":["# Load pre-trained model from TensorFlow Hub\n","hub_url = \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature_vector/4\"\n","hub_model = hub.KerasLayer(hub_url, trainable=False)\n","\n","model = tf.keras.Sequential([\n","    hub_model,                          # obtain feature vector from a pre-trained neural network  \n","    tf.keras.layers.Dense(NUM_CLASSES)  # classification layer. number of units = number of classes\n","])\n","model.build([None, IMG_HEIGHT, IMG_WIDTH, 3])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnpnqDxIcIqv"},"source":["model.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=['acc']\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QS9rPxV819Qw"},"source":["history = model.fit(\n","    train100_ds,\n","    epochs=10\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVtlXojmKkQK"},"source":["history_data = pd.DataFrame(history.history)\n","\n","history_data.plot(y=['loss'])\n","history_data.plot(y=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMKEEZZhK4NF"},"source":["model.evaluate(test_ds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IP3WjoCVxUVf"},"source":["# confusion matrix\n","logits = model.predict(test_ds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVvJYxVDclMQ"},"source":["probs = tf.nn.softmax(logits)\n","preds = np.argmax(probs.numpy(), axis=1)\n","\n","conf_matrix = tf.math.confusion_matrix(labels, preds)\n","\n","plt.figure(figsize=(10,8))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', square=True, xticklabels=CLASSES, yticklabels=CLASSES)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIyt1b0X2YfC"},"source":["logits = model(test_img)\n","probs = tf.nn.softmax(logits).numpy()[0]\n","\n","table = pd.DataFrame({'class': list(CLASSES), 'probability': probs})\n","table['probability'] = table['probability'].map('{:5f}'.format)\n","table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y4-T0gpc_adn"},"source":["# (Optional) Visualize feature vectors"]},{"cell_type":"code","metadata":{"id":"VehwLRzw4ylI"},"source":["import os\n","train_df = pd.DataFrame()\n","\n","for class_name in CLASSES:\n","    files = os.listdir(os.path.join(TRAIN_PATH, class_name))\n","    df = pd.DataFrame({'filename': files, 'class': class_name})\n","    df['path'] = TRAIN_PATH + '/' + class_name + '/' + df['filename']\n","    train_df = train_df.append(df, ignore_index=True)\n","\n","# prepare a TensorFlow Dataset with only image data (no label)\n","def import_img(path):\n","    img = tf.io.read_file(path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.convert_image_dtype(img, tf.float32)\n","    img = tf.image.resize(img, (224,224))\n","    return img\n","\n","train_inputs = tf.data.Dataset.from_tensor_slices(train_df['path']).cache()\n","train_inputs = train_inputs.map(import_img, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYggMVAP3QvH"},"source":["# build a model with only feature vector ouput\n","hub_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n","hub_model = hub.KerasLayer(hub_url, trainable=False)\n","feature_model = tf.keras.Sequential(hub_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_FZP_UycQe-"},"source":["features = feature_model.predict(train_inputs)\n","print(features.shape)  # shape (num_examples, vector_dimension)\n","features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7yVhMtf_n_S"},"source":["The features are in a vector space with 2048 dimensions. To visualize it in 3D space, we will reduce the dimensionality with Principal Component Analysis (PCA)"]},{"cell_type":"code","metadata":{"id":"xkCx25mh3jc_"},"source":["from sklearn.decomposition import PCA\n","pca = PCA(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SwuJ8zjD33C5"},"source":["pca.fit(features)\n","features_3d = pca.transform(features)\n","\n","# shape (num_examples, 3)\n","print(features_3d.shape)\n","features_3d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cWnwhKSm9Z_e"},"source":["import plotly.express as px\n","\n","x = features_3d[:,0]\n","y = features_3d[:,1]\n","z = features_3d[:,2]\n","\n","fig = px.scatter_3d(x=x, y=y, z=z, color=train_df['class'])\n","fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ij3_bGx2A-ti"},"source":["img_feature = feature_model(test_img)\n","img_feature = pca.transform(img_feature)\n","\n","x_img = np.append(x, img_feature[0,0])\n","y_img = np.append(y, img_feature[0,1])\n","z_img = np.append(z, img_feature[0,2])\n","color_img = np.append(train_df['class'].values, 'Test image')\n","size_img = [1] * len(train_df) + [10]\n","\n","fig = px.scatter_3d(\n","    x=x_img, \n","    y=y_img, \n","    z=z_img, \n","    color=color_img,\n","    size=size_img\n",")\n","\n","fig.update_layout(\n","    scene=dict(\n","        annotations=[dict(\n","            # showarrow=False,\n","            x=img_feature[0,0],\n","            y=img_feature[0,1],\n","            z=img_feature[0,2],\n","            text=\"Test image\",\n","        )]\n","    ),\n",")\n","fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p00NrNCux2fI"},"source":["# What's next?\n","\n","So you have learned the basics of Transfer Learning, and how to use TensorFlow Hub to get the latest ML models. But what can you do with your newly trained model?\n","\n","## Persistent storage, save weights and load weights\n","\n","After you close Google Colab, all of your data is lost. One way to save your progress is to save to Google Drive.\n","\n","TensorFlow models can save and load weights with `.save_weights()` and `.load_weights()` methods.\n","\n","From Google Colab, you can mount your Google Drive to Colab instance, and save and load weights directly to and from your Google Drive.\n","\n","For more information\n","- https://www.tensorflow.org/tutorials/keras/save_and_load#manually_save_weights\n","\n","## Deployment - TensorFlow Serving, TensorFlow Lite\n","\n","Export your model in the `SavedModel` format and serve it with TensorFlow Serving! You now have a web API for you mobile and web applications to use your ML model\n","\n","But you want your model to be on your phone, or in a small, portable device like those Safe Entry temperature scanner. Just convert your model to the TensorFlow Lite format, and you are good to go!\n","\n","*Shhh the details are actually a bit more complicated than that, but the idea is there. You can read more about it below:*\n","- https://www.tensorflow.org/tfx/serving/serving_basic\n","- https://www.tensorflow.org/lite/guide\n","\n"]}]}